{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import time\n",
    "import torchtext\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embvec = torchtext.vocab.GloVe(name='840B', dim=300,cache='/home/pding/Documents/glove/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "svoc = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC</th>\n",
       "      <th>TRG</th>\n",
       "      <th>keywords</th>\n",
       "      <th>ext perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32460167</th>\n",
       "      <td>Clinical implications of serum neurofilament i...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O B ...</td>\n",
       "      <td>[Biomarker, Multiple sclerosis, Neurofilament ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32460145</th>\n",
       "      <td>Effect of cognitive rehabilitation on neuropsy...</td>\n",
       "      <td>O O B I O O O O O O O O O O O B I O O O O O O ...</td>\n",
       "      <td>[Clinical trial, Cognition, Cognitive rehabili...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32460086</th>\n",
       "      <td>B-cell depleting therapies may affect suscepti...</td>\n",
       "      <td>B I I I I O O O O O O O O O O B I O O O B O O ...</td>\n",
       "      <td>[B-cell depleting therapies, COVID-19, DMTs, M...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32459513</th>\n",
       "      <td>The dichotomous role of the gut microbiome in ...</td>\n",
       "      <td>O O O O O B I O O O O B I O O O B I O O O O O ...</td>\n",
       "      <td>[Alzheimer’s disease, Parkinson’s disease, gut...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32458728</th>\n",
       "      <td>Frailty in ageing persons with multiple sclero...</td>\n",
       "      <td>B O B O O B I O O O O B I O O O O O O O O O O ...</td>\n",
       "      <td>[Fried’s phenotype, Multiple sclerosis, ageing...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7441643</th>\n",
       "      <td>Ghana--medical care amid economic problems. De...</td>\n",
       "      <td>B O O O O O O O O O O O O O O B B I O O O O B ...</td>\n",
       "      <td>[Africa, Africa South Of The Sahara, Delivery ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310887</th>\n",
       "      <td>Oral contraceptive therapy and systemic lupus ...</td>\n",
       "      <td>O O O O B I I O O O O O O O O B I I O O O O O ...</td>\n",
       "      <td>[Biology, Case Studies, Contraception, Contrac...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197509</th>\n",
       "      <td>Estrogens for the menopause. Maximizing benefi...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O O O ...</td>\n",
       "      <td>[Androgens--administraction and dosage, Biolog...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14312444</th>\n",
       "      <td>PORPHOBILINOGEN AND DELTA-AMINOLEVULINIC ACID ...</td>\n",
       "      <td>O O O O O O B O B I O O B O O O O O O O O O B ...</td>\n",
       "      <td>[AMINO ACIDS, DEMYELINATION, DIAGNOSIS, DIFFER...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14280011</th>\n",
       "      <td>FUNCTIONAL STUDIES OF CULTURED BRAIN TISSUES A...</td>\n",
       "      <td>O O O O O O O O O O O O O O O O O O O O O B O ...</td>\n",
       "      <td>[ANIMALS, NEWBORN, BLOOD, BRAIN ELECTROPHYSIOL...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14428 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        SRC  \\\n",
       "32460167  Clinical implications of serum neurofilament i...   \n",
       "32460145  Effect of cognitive rehabilitation on neuropsy...   \n",
       "32460086  B-cell depleting therapies may affect suscepti...   \n",
       "32459513  The dichotomous role of the gut microbiome in ...   \n",
       "32458728  Frailty in ageing persons with multiple sclero...   \n",
       "...                                                     ...   \n",
       "7441643   Ghana--medical care amid economic problems. De...   \n",
       "310887    Oral contraceptive therapy and systemic lupus ...   \n",
       "197509    Estrogens for the menopause. Maximizing benefi...   \n",
       "14312444  PORPHOBILINOGEN AND DELTA-AMINOLEVULINIC ACID ...   \n",
       "14280011  FUNCTIONAL STUDIES OF CULTURED BRAIN TISSUES A...   \n",
       "\n",
       "                                                        TRG  \\\n",
       "32460167  O O O O O O O O O O O O O O O O O O O O O O B ...   \n",
       "32460145  O O B I O O O O O O O O O O O B I O O O O O O ...   \n",
       "32460086  B I I I I O O O O O O O O O O B I O O O B O O ...   \n",
       "32459513  O O O O O B I O O O O B I O O O B I O O O O O ...   \n",
       "32458728  B O B O O B I O O O O B I O O O O O O O O O O ...   \n",
       "...                                                     ...   \n",
       "7441643   B O O O O O O O O O O O O O O B B I O O O O B ...   \n",
       "310887    O O O O B I I O O O O O O O O B I I O O O O O ...   \n",
       "197509    O O O O O O O O O O O O O O O O O O O O O O O ...   \n",
       "14312444  O O O O O O B O B I O O B O O O O O O O O O B ...   \n",
       "14280011  O O O O O O O O O O O O O O O O O O O O O B O ...   \n",
       "\n",
       "                                                   keywords ext perc  \n",
       "32460167  [Biomarker, Multiple sclerosis, Neurofilament ...        4  \n",
       "32460145  [Clinical trial, Cognition, Cognitive rehabili...        4  \n",
       "32460086  [B-cell depleting therapies, COVID-19, DMTs, M...        4  \n",
       "32459513  [Alzheimer’s disease, Parkinson’s disease, gut...        5  \n",
       "32458728  [Fried’s phenotype, Multiple sclerosis, ageing...        5  \n",
       "...                                                     ...      ...  \n",
       "7441643   [Africa, Africa South Of The Sahara, Delivery ...        5  \n",
       "310887    [Biology, Case Studies, Contraception, Contrac...        3  \n",
       "197509    [Androgens--administraction and dosage, Biolog...        7  \n",
       "14312444  [AMINO ACIDS, DEMYELINATION, DIAGNOSIS, DIFFER...        6  \n",
       "14280011  [ANIMALS, NEWBORN, BLOOD, BRAIN ELECTROPHYSIOL...        6  \n",
       "\n",
       "[14428 rows x 4 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datao = pd.read_pickle(\"~/OneDrive/kph/processed2.pkl\")\n",
    "datatrain = datao[datao['ext perc']>=3]\n",
    "datatest = datao[datao['ext perc']<3]\n",
    "# separate train and validate \n",
    "VAL_RATIO = 0.2\n",
    "dtrain = datatrain.loc[:,['SRC','TRG']]\n",
    "dtraink = datatrain.loc[:,['SRC','TRG','keywords']]\n",
    "seed=250\n",
    "idx = np.arange(datatrain.shape[0])\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(idx)\n",
    "val_size = int(len(idx) * VAL_RATIO)\n",
    "df_train = dtrain.iloc[idx[val_size:], :]\n",
    "df_val = dtrain.iloc[idx[:val_size], :]\n",
    "df_val_k = dtraink.iloc[idx[:val_size], :]\n",
    "df_test = datatest.loc[:,['SRC','TRG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtraink = datatrain.loc[:,['SRC','TRG','keywords']]\n",
    "df_val_k = dtraink.iloc[idx[:val_size], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizertrg = lambda x: x.split()\n",
    "def tokenizersrc(text): # create a tokenizer function\n",
    "    return [tok.text for tok in svoc.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(corpus_file, datafields):\n",
    "    examples = []\n",
    "    words = []\n",
    "    labels = []\n",
    "    for pmid in df_train.index:\n",
    "        words = tokenizersrc(df_train.loc[pmid,'SRC'])\n",
    "        labels = tokenizertrg(df_train.loc[pmid,'TRG'])\n",
    "        examples.append(torchtext.data.Example.fromlist([words, labels], datafields))\n",
    "    return torchtext.data.Dataset(examples, datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_field, label_field, emb_dim, rnn_size, update_pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        voc_size = len(text_field.vocab)\n",
    "        self.n_labels = len(label_field.vocab)       \n",
    "        \n",
    "        # Embedding layer. If we're using pre-trained embeddings, copy them\n",
    "        # into our embedding module.\n",
    "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
    "        if text_field.vocab.vectors is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(text_field.vocab.vectors, \n",
    "                                                       requires_grad=update_pretrained)\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU with one layer.\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_size, \n",
    "                          bidirectional=True, num_layers=2)\n",
    "\n",
    "        # Output layer. As in the example last week, the input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(2*rnn_size, self.n_labels)\n",
    " \n",
    "        # To deal with the padding positions later, we need to know the\n",
    "        # encoding of the padding dummy word and the corresponding dummy output tag.\n",
    "        self.pad_word_id = text_field.vocab.stoi[text_field.pad_token]\n",
    "        self.pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "    def compute_outputs(self, sentences):\n",
    "        # The words in the documents are encoded as integers. The shape of the documents\n",
    "        # tensor is (max_len, n_docs), where n_docs is the number of documents in this batch,\n",
    "        # and max_len is the maximal length of a document in the batch.\n",
    "\n",
    "        # First look up the embeddings for all the words in the documents.\n",
    "        # The shape is now (max_len, n_sentences, emb_dim).        \n",
    "        embedded = self.embedding(sentences)\n",
    "\n",
    "        # Apply the RNN.\n",
    "        # The shape of the RNN output tensor is (max_len, n_sentences, 2*rnn_size).\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Apply the linear output layer.\n",
    "        # The shape of the output tensor is (max_len, n_sentences, n_labels).\n",
    "        out = self.top_layer(rnn_out)\n",
    "        \n",
    "        # Find the positions where the token is a dummy padding token.\n",
    "        pad_mask = (sentences == self.pad_word_id).float()\n",
    "\n",
    "        # For these positions, we add some large number in the column corresponding\n",
    "        # to the dummy padding label.\n",
    "        out[:, :, self.pad_label_id] += pad_mask*10000\n",
    "\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels):\n",
    "        # As discussed above, this method first computes the predictions, and then\n",
    "        # the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        # We transpose the prediction to (n_sentences, max_len), and convert it\n",
    "        # to a NumPy matrix.\n",
    "        return predicted.t().cpu().numpy()\n",
    "\n",
    "from torchcrf import CRF\n",
    "\n",
    "class RNNCRFTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_field, label_field, emb_dim, rnn_size, update_pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        voc_size = len(text_field.vocab)\n",
    "        self.n_labels = len(label_field.vocab)       \n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
    "        if text_field.vocab.vectors is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(text_field.vocab.vectors, \n",
    "                                                       requires_grad=update_pretrained)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=emb_dim, hidden_size=rnn_size, \n",
    "                          bidirectional=True, num_layers=1)\n",
    "\n",
    "        self.top_layer = nn.Linear(2*rnn_size, self.n_labels)\n",
    " \n",
    "        self.pad_word_id = text_field.vocab.stoi[text_field.pad_token]\n",
    "        self.pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
    "    \n",
    "        self.crf = CRF(self.n_labels)\n",
    "        \n",
    "    def compute_outputs(self, sentences):\n",
    "        embedded = self.embedding(sentences)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        out = self.top_layer(rnn_out)\n",
    "        \n",
    "        pad_mask = (sentences == self.pad_word_id).float()\n",
    "        out[:, :, self.pad_label_id] += pad_mask*10000\n",
    "        \n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels):\n",
    "        # Compute the outputs of the lower layers, which will be used as emission\n",
    "        # scores for the CRF.\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # We return the loss value. The CRF returns the log likelihood, but we return \n",
    "        # the *negative* log likelihood as the loss value.            \n",
    "        # PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\n",
    "        # log likelihood.\n",
    "        return -self.crf(scores, labels)\n",
    "            \n",
    "    def predict(self, sentences):\n",
    "        # Compute the emission scores, as above.\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # Apply the Viterbi algorithm to get the predictions. This implementation returns\n",
    "        # the result as a list of lists (not a tensor), corresponding to a matrix\n",
    "        # of shape (n_sentences, max_len).\n",
    "        return self.crf.decode(scores)\n",
    "# Convert a list of BIO labels, coded as integers, into spans identified by a beginning, an end, and a label.\n",
    "# To allow easy comparison later, we store them in a dictionary indexed by the start position.\n",
    "def to_spans(l_ids, voc):\n",
    "    spans = {}\n",
    "    current_lbl = None\n",
    "    current_start = None\n",
    "    for i, l_id in enumerate(l_ids):\n",
    "        l = voc[l_id]\n",
    "\n",
    "        if l[0] == 'B': \n",
    "            # Beginning of a named entity: B-something.\n",
    "            if current_lbl:\n",
    "                # If we're working on an entity, close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "            # Create a new entity that starts here.\n",
    "            current_lbl = l[2:]\n",
    "            current_start = i\n",
    "        elif l[0] == 'I':\n",
    "            # Continuation of an entity: I-something.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, but its label does not\n",
    "                # correspond to the predicted I-tag, then we close\n",
    "                # the open entity and create a new one.\n",
    "                if current_lbl != l[2:]:\n",
    "                    spans[current_start] = (current_lbl, i)\n",
    "                    current_lbl = l[2:]\n",
    "                    current_start = i\n",
    "            else:\n",
    "                # If we don't have an open entity but predict an I tag,\n",
    "                # we create a new entity starting here even though we're\n",
    "                # not following the format strictly.\n",
    "                current_lbl = l[2:]\n",
    "                current_start = i\n",
    "        else:\n",
    "            # Outside: O.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, we close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "                current_lbl = None\n",
    "                current_start = None\n",
    "    return spans\n",
    "\n",
    "# Compares two sets of spans and records the results for future aggregation.\n",
    "def compare(gold, pred, stats):\n",
    "    for start, (lbl, end) in gold.items():\n",
    "        stats['total']['gold'] += 1\n",
    "        stats[lbl]['gold'] += 1\n",
    "    for start, (lbl, end) in pred.items():\n",
    "        stats['total']['pred'] += 1\n",
    "        stats[lbl]['pred'] += 1\n",
    "    for start, (glbl, gend) in gold.items():\n",
    "        if start in pred:\n",
    "            plbl, pend = pred[start]\n",
    "            if glbl == plbl and gend == pend:\n",
    "                stats['total']['corr'] += 1\n",
    "                stats[glbl]['corr'] += 1\n",
    "\n",
    "# This function combines the auxiliary functions we defined above.\n",
    "def evaluate_iob(predicted, gold, label_field, stats):\n",
    "    # The gold-standard labels are assumed to be an integer tensor of shape\n",
    "    # (max_len, n_sentences), as returned by torchtext.\n",
    "    gold_cpu = gold.t().cpu().numpy()\n",
    "    gold_cpu = list(gold_cpu.reshape(-1))\n",
    "\n",
    "    # The predicted labels assume the format produced by pytorch-crf, so we\n",
    "    # assume that they have been converted into a list already.\n",
    "    # We just flatten the list.\n",
    "    pred_cpu = [l for sen in predicted for l in sen]\n",
    "    \n",
    "    # Compute spans for the gold standard and prediction.\n",
    "    gold_spans = to_spans(gold_cpu, label_field.vocab.itos)\n",
    "    pred_spans = to_spans(pred_cpu, label_field.vocab.itos)\n",
    "\n",
    "    # Finally, update the counts for correct, predicted and gold-standard spans.\n",
    "    compare(gold_spans, pred_spans, stats)\n",
    "\n",
    "# Computes precision, recall and F-score, given a dictionary that contains\n",
    "# the counts of correct, predicted and gold-standard items.\n",
    "def prf(stats):\n",
    "    if stats['pred'] == 0:\n",
    "        return 0, 0, 0\n",
    "    p = stats['corr']/stats['pred']\n",
    "    r = stats['corr']/stats['gold']\n",
    "    if p > 0 and r > 0:\n",
    "        f = 2*p*r/(p+r)\n",
    "    else:\n",
    "        f = 0\n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, lower=False)\n",
    "LABEL = torchtext.data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, unk_token=None)\n",
    "fields = [('text', TEXT), ('label', LABEL)]\n",
    "device = 'cuda'\n",
    "use_pretrained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using pre-trained word embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNCRFTagger(\n",
       "  (embedding): Embedding(72288, 300)\n",
       "  (rnn): LSTM(300, 128, bidirectional=True)\n",
       "  (top_layer): Linear(in_features=256, out_features=6, bias=True)\n",
       "  (crf): CRF(num_tags=6)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples = read_data(df_train, fields)\n",
    "valid_examples = read_data(df_val, fields)\n",
    "# Load the pre-trained embeddings that come with the torchtext library.\n",
    "if use_pretrained:\n",
    "    print('We are using pre-trained word embeddings.')\n",
    "    TEXT.build_vocab(train_examples, vectors=embvec)\n",
    "else:  \n",
    "    print('We are training word embeddings from scratch.')\n",
    "    TEXT.build_vocab(train_examples, max_size=5000)\n",
    "LABEL.build_vocab(train_examples)\n",
    "# Create one of the models defined above.\n",
    "#self.model = RNNTagger(self.TEXT, self.LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "model0 = RNNCRFTagger(TEXT, LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "\n",
    "model0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 310\n",
    "\n",
    "optimizer = torch.optim.Adam(model0.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "n_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_examples, valid_examples, embvec, TEXT, LABEL, device, model, batch_size, optimizer, n_epochs):\n",
    "\n",
    "\n",
    "    # Count the number of words and sentences.\n",
    "    n_tokens_train = 0\n",
    "    n_sentences_train = 0\n",
    "    for ex in train_examples:\n",
    "        n_tokens_train += len(ex.text) + 2\n",
    "        n_sentences_train += 1\n",
    "    n_tokens_valid = 0       \n",
    "    for ex in valid_examples:\n",
    "        n_tokens_valid += len(ex.text)\n",
    "\n",
    "\n",
    "    \n",
    "    n_batches = np.ceil(n_sentences_train / batch_size)\n",
    "\n",
    "    mean_n_tokens = n_tokens_train / n_batches\n",
    "\n",
    "    train_iterator = torchtext.data.BucketIterator(\n",
    "        train_examples,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        repeat=False,\n",
    "        train=True,\n",
    "        sort=True)\n",
    "\n",
    "    valid_iterator = torchtext.data.BucketIterator(\n",
    "        valid_examples,\n",
    "        device=device,\n",
    "        batch_size=64,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        repeat=False,\n",
    "        train=False,\n",
    "        sort=True)\n",
    "\n",
    "    train_batches = list(train_iterator)\n",
    "    valid_batches = list(valid_iterator)\n",
    "\n",
    "    n_labels = len(LABEL.vocab)\n",
    "\n",
    "    history = defaultdict(list)    \n",
    "\n",
    "   \n",
    "\n",
    "    for i in range(1, n_epochs + 1):\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        loss_sum = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch in train_batches:\n",
    "\n",
    "            # Compute the output and loss.\n",
    "            loss = model(batch.text, batch.label) / mean_n_tokens\n",
    "\n",
    "            optimizer.zero_grad()            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        train_loss = loss_sum / n_batches\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        # Evaluate on the validation set.\n",
    "        if i % 1 == 0:\n",
    "            stats = defaultdict(Counter)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch in valid_batches:\n",
    "                    # Predict the model's output on a batch.\n",
    "                    predicted = model.predict(batch.text)                   \n",
    "                    # Update the evaluation statistics.\n",
    "                    evaluate_iob(predicted, batch.label, LABEL, stats)\n",
    "\n",
    "            # Compute the overall F-score for the validation set.\n",
    "            _, _, val_f1 = prf(stats['total'])\n",
    "\n",
    "            history['val_f1'].append(val_f1)\n",
    "\n",
    "            t1 = time.time()\n",
    "            print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "    # After the final evaluation, we print more detailed evaluation statistics, including\n",
    "    # precision, recall, and F-scores for the different types of named entities.\n",
    "    print()\n",
    "    print('Final evaluation on the validation set:')\n",
    "    p, r, f1 = prf(stats['total'])\n",
    "    print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "    for label in stats:\n",
    "        if label != 'total':\n",
    "            p, r, f1 = prf(stats[label])\n",
    "            print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "\n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.plot(history['val_f1'])\n",
    "    plt.legend(['training loss', 'validation F-score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss = 0.0779, val f1: 0.0000, time = 76.9721\n",
      "Epoch 2: train loss = 0.0771, val f1: 0.0000, time = 76.9538\n",
      "Epoch 3: train loss = 0.0767, val f1: 0.0000, time = 77.0305\n",
      "Epoch 4: train loss = 0.0773, val f1: 0.0000, time = 76.8683\n",
      "Epoch 5: train loss = 0.0772, val f1: 0.0000, time = 77.1605\n",
      "Epoch 6: train loss = 0.0769, val f1: 0.0000, time = 77.0128\n",
      "Epoch 7: train loss = 0.0768, val f1: 0.0000, time = 76.7918\n",
      "Epoch 8: train loss = 0.0752, val f1: 0.0000, time = 76.9088\n",
      "Epoch 9: train loss = 0.0745, val f1: 0.0000, time = 76.1056\n",
      "Epoch 10: train loss = 0.0738, val f1: 0.0000, time = 76.1435\n",
      "Epoch 11: train loss = 0.0732, val f1: 0.0000, time = 76.0781\n",
      "Epoch 12: train loss = 0.0727, val f1: 0.0000, time = 76.0442\n",
      "Epoch 13: train loss = 0.0724, val f1: 0.0000, time = 76.1555\n",
      "Epoch 14: train loss = 0.0725, val f1: 0.0000, time = 76.1489\n",
      "Epoch 15: train loss = 0.0724, val f1: 0.0000, time = 76.2030\n",
      "\n",
      "Final evaluation on the validation set:\n",
      "Overall: P = 0.0000, R = 0.0000, F1 = 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 329,
       "width": 487
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(train_examples, valid_examples, embvec, TEXT, LABEL, device, model0, batch_size, optimizer,n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model0.state_dict(), '/home/pding/OneDrive/kph/kph/lstmcrflow.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model0.state_dict(), '/home/pding/OneDrive/kph/kph/lstmcr724.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for RNNCRFTagger:\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([384, 300]) from checkpoint, the shape in current model is torch.Size([512, 300]).\n\tsize mismatch for rnn.weight_hh_l0: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for rnn.bias_ih_l0: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for rnn.bias_hh_l0: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([384, 300]) from checkpoint, the shape in current model is torch.Size([512, 300]).\n\tsize mismatch for rnn.weight_hh_l0_reverse: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for rnn.bias_ih_l0_reverse: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for rnn.bias_hh_l0_reverse: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-93ede9092dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/pding/OneDrive/kph//kph/lstmcrf.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/kph2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m    847\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RNNCRFTagger:\n\tsize mismatch for rnn.weight_ih_l0: copying a param with shape torch.Size([384, 300]) from checkpoint, the shape in current model is torch.Size([512, 300]).\n\tsize mismatch for rnn.weight_hh_l0: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for rnn.bias_ih_l0: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for rnn.bias_hh_l0: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for rnn.weight_ih_l0_reverse: copying a param with shape torch.Size([384, 300]) from checkpoint, the shape in current model is torch.Size([512, 300]).\n\tsize mismatch for rnn.weight_hh_l0_reverse: copying a param with shape torch.Size([384, 128]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for rnn.bias_ih_l0_reverse: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for rnn.bias_hh_l0_reverse: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([512])."
     ]
    }
   ],
   "source": [
    "model0.load_state_dict(torch.load('/home/pding/OneDrive/kph/kph/lstmcrf.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger:\n",
    "    \n",
    "    def __init__(self, lower):\n",
    "        self.TEXT = torchtext.data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, lower=lower)\n",
    "        self.LABEL = torchtext.data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, unk_token=None)\n",
    "        self.fields = [('text', self.TEXT), ('label', self.LABEL)]\n",
    "        self.device = 'cuda'\n",
    "        \n",
    "    def tag(self, sentences):\n",
    "        # This method applies the trained model to a list of sentences.\n",
    "        \n",
    "        # First, create a torchtext Dataset containing the sentences to tag.\n",
    "        examples = []\n",
    "        for sen in sentences:\n",
    "            labels = ['O']*len(sen) # placeholder\n",
    "            examples.append(torchtext.data.Example.fromlist([sen, labels], self.fields))\n",
    "        dataset = torchtext.data.Dataset(examples, self.fields)\n",
    "        \n",
    "        iterator = torchtext.data.Iterator(\n",
    "            dataset,\n",
    "            device=self.device,\n",
    "            batch_size=64,\n",
    "            repeat=False,\n",
    "            train=False,\n",
    "            sort=False)\n",
    "        \n",
    "        # Apply the trained model to all batches.\n",
    "        out = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in iterator:\n",
    "                # Call the model's predict method. This returns a list of NumPy matrix\n",
    "                # containing the integer-encoded tags for each sentence.\n",
    "                predicted = self.model.predict(batch.text)\n",
    "\n",
    "                # Convert the integer-encoded tags to tag strings.\n",
    "                for tokens, pred_sen in zip(sentences, predicted):\n",
    "                    out.append([self.LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "        return out\n",
    "                \n",
    "    def train(self):\n",
    "        # Read training and validation data according to the predefined split.\n",
    "        train_examples = read_data(df_train, self.fields)\n",
    "        valid_examples = read_data(df_val, self.fields)\n",
    "\n",
    "        # Count the number of words and sentences.\n",
    "        n_tokens_train = 0\n",
    "        n_sentences_train = 0\n",
    "        for ex in train_examples:\n",
    "            n_tokens_train += len(ex.text) + 2\n",
    "            n_sentences_train += 1\n",
    "        n_tokens_valid = 0       \n",
    "        for ex in valid_examples:\n",
    "            n_tokens_valid += len(ex.text)\n",
    "\n",
    "        # Load the pre-trained embeddings that come with the torchtext library.\n",
    "        use_pretrained = True\n",
    "        if use_pretrained:\n",
    "            print('We are using pre-trained word embeddings.')\n",
    "            self.TEXT.build_vocab(train_examples, vectors=embvec)\n",
    "        else:  \n",
    "            print('We are training word embeddings from scratch.')\n",
    "            self.TEXT.build_vocab(train_examples, max_size=5000)\n",
    "        self.LABEL.build_vocab(train_examples)\n",
    "    \n",
    "        # Create one of the models defined above.\n",
    "        #self.model = RNNTagger(self.TEXT, self.LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "        self.model = RNNCRFTagger(self.TEXT, self.LABEL, emb_dim=300, rnn_size=128, update_pretrained=False)\n",
    "    \n",
    "        self.model.to(self.device)\n",
    "    \n",
    "        batch_size = 310\n",
    "        n_batches = np.ceil(n_sentences_train / batch_size)\n",
    "\n",
    "        mean_n_tokens = n_tokens_train / n_batches\n",
    "\n",
    "        train_iterator = torchtext.data.BucketIterator(\n",
    "            train_examples,\n",
    "            device=self.device,\n",
    "            batch_size=batch_size,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            train=True,\n",
    "            sort=True)\n",
    "\n",
    "        valid_iterator = torchtext.data.BucketIterator(\n",
    "            valid_examples,\n",
    "            device=self.device,\n",
    "            batch_size=64,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            repeat=False,\n",
    "            train=False,\n",
    "            sort=True)\n",
    "    \n",
    "        train_batches = list(train_iterator)\n",
    "        valid_batches = list(valid_iterator)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "        n_labels = len(self.LABEL.vocab)\n",
    "\n",
    "        history = defaultdict(list)    \n",
    "        \n",
    "        n_epochs = 15\n",
    "        \n",
    "        for i in range(1, n_epochs + 1):\n",
    "\n",
    "            t0 = time.time()\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "            self.model.train()\n",
    "            for batch in train_batches:\n",
    "                \n",
    "                # Compute the output and loss.\n",
    "                loss = self.model(batch.text, batch.label) / mean_n_tokens\n",
    "                \n",
    "                optimizer.zero_grad()            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_sum += loss.item()\n",
    "\n",
    "            train_loss = loss_sum / n_batches\n",
    "            history['train_loss'].append(train_loss)\n",
    "\n",
    "            # Evaluate on the validation set.\n",
    "            if i % 1 == 0:\n",
    "                stats = defaultdict(Counter)\n",
    "\n",
    "                self.model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for batch in valid_batches:\n",
    "                        # Predict the model's output on a batch.\n",
    "                        predicted = self.model.predict(batch.text)                   \n",
    "                        # Update the evaluation statistics.\n",
    "                        evaluate_iob(predicted, batch.label, self.LABEL, stats)\n",
    "            \n",
    "                # Compute the overall F-score for the validation set.\n",
    "                _, _, val_f1 = prf(stats['total'])\n",
    "                \n",
    "                history['val_f1'].append(val_f1)\n",
    "            \n",
    "                t1 = time.time()\n",
    "                print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "           \n",
    "        # After the final evaluation, we print more detailed evaluation statistics, including\n",
    "        # precision, recall, and F-scores for the different types of named entities.\n",
    "        print()\n",
    "        print('Final evaluation on the validation set:')\n",
    "        p, r, f1 = prf(stats['total'])\n",
    "        print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        for label in stats:\n",
    "            if label != 'total':\n",
    "                p, r, f1 = prf(stats[label])\n",
    "                print(f'{label:4s}: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')\n",
    "        \n",
    "        plt.plot(history['train_loss'])\n",
    "        plt.plot(history['val_f1'])\n",
    "        plt.legend(['training loss', 'validation F-score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = Tagger(lower=False)\n",
    "#tagger.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagger2(sentences, model):\n",
    "    # This method applies the trained model to a list of sentences.\n",
    "\n",
    "    # First, create a torchtext Dataset containing the sentences to tag.\n",
    "    examples = []\n",
    "    for sen in sentences:\n",
    "        words = tokenizersrc(sen)\n",
    "        labels = ['O']*len(words) # placeholder\n",
    "        examples.append(torchtext.data.Example.fromlist([words, labels], fields))\n",
    "    dataset = torchtext.data.Dataset(examples, fields)\n",
    "\n",
    "    iterator = torchtext.data.Iterator(\n",
    "        dataset,\n",
    "        device=device,\n",
    "        batch_size=64,\n",
    "        repeat=False,\n",
    "        train=False,\n",
    "        sort=False)\n",
    "\n",
    "    # Apply the trained model to all batches.\n",
    "    out = []\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        # Call the model's predict method. This returns a list of NumPy matrix\n",
    "        # containing the integer-encoded tags for each sentence.\n",
    "        predicted = model.predict(batch.text)\n",
    "\n",
    "        # Convert the integer-encoded tags to tag strings.\n",
    "        #for tokens, pred_sen in zip(sentences, predicted):\n",
    "        for tokens, pred_sen in zip(sentences, predicted):\n",
    "            out.append([LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kphext(sentences,tags):\n",
    "    kph = []\n",
    "    for i in range(len(sentences)):\n",
    "        s0 = svoc.tokenizer(sentences[i])\n",
    "        s1 = [tok.text for tok in s0]\n",
    "        t1 = tags[i]\n",
    "        k1 = []\n",
    "        for j in range(len(s1)):\n",
    "            if t1[j] == 'O':\n",
    "                k2 = ''\n",
    "            if t1[j] == 'B':\n",
    "                k2 = str(s1[j])\n",
    "                try: \n",
    "                    kt = t1[j+1]\n",
    "                    if kt == 'B':\n",
    "                        k1.append(k2)\n",
    "                    if kt == 'O':\n",
    "                        k1.append(k2)\n",
    "                except(AttributeError, TypeError):\n",
    "                    pass\n",
    "            if t1[j] == 'I':\n",
    "                k2 = k2 + ' '+str(s1[j])\n",
    "                try: \n",
    "                    kt = t1[j+1]\n",
    "                    if kt == 'B':\n",
    "                        k1.append(k2)\n",
    "                    if kt == 'O':\n",
    "                        k1.append(k2)\n",
    "                except(AttributeError, TypeError):\n",
    "                    pass\n",
    "        kph.append(k1)\n",
    "    return kph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kphext2(sentences,tags):\n",
    "    kph = []\n",
    "    for i in range(len(sentences)):\n",
    "        s0 = svoc.tokenizer(sentences[i])\n",
    "        s1 = [tok.text for tok in s0]\n",
    "        t1 = tags[i]\n",
    "        k1 = []\n",
    "        for j in range(len(s1)):\n",
    "            start = j\n",
    "            if t1[j] == 'B':\n",
    "                sti = 0\n",
    "                stop = j+1\n",
    "                while sti == 0:\n",
    "                    try: \n",
    "                        kt = str(t1[stop])\n",
    "                        if kt == 'I':\n",
    "                            stop = stop+1\n",
    "                        else:\n",
    "                            k2 = str(s0[start:stop])\n",
    "                            k1.append(k2)\n",
    "                            sti =1\n",
    "                    except(IndexError):\n",
    "                        k2 = s0[start:stop]\n",
    "                        k1.append(k2)\n",
    "                        sti =1\n",
    "                k2 = str(s1[j])\n",
    "        kph.append(k1)\n",
    "    return kph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tagger' object has no attribute 'train_examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-f94aab141392>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tagger' object has no attribute 'train_examples'"
     ]
    }
   ],
   "source": [
    "tagger.train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = ['Case studies illustrating the management of trigeminal neuropathic pain using topical 5% lidocaine plasters. Chronic trigeminal pain, with its severe related functional problems, is difficult to treat. Treatment is often empirically based on medications used for other chronic pain conditions. Systemic sodium channel and calcium channel blocking agents may cause a multitude of complications that are often poorly tolerated by the patient.\\nThe aim of this case report was to assess the efficacy of topical 5% lidocaine plasters in reducing pain and reducing adjuvant medication in patients with orofacial neuropathic pain.\\nFourteen patients with chronic orofacial pain conditions referred to the oral surgery department were instructed to wear 5% lidocaine plasters for 12 hours each day over the painful area. The conditions included post-surgical neuropathy (n = 10), multiple sclerosis-related pain (n = 1), persistent idiopathic facial pain (n = 1), Ramsay Hunt syndrome (post-herpetic neuralgia, n = 1) and trigeminal neuralgia (n = 1). Data were collected on patient demographics, pain levels and medication.\\nPain levels improved in 12 out of 14 patients. Nine patients had a reduction in adjuvant medication, two of whom completely stopped adjuvant treatment.\\nThis case series demonstrates that of the use of 5% lidocaine plasters may play a useful role in the management of chronic trigeminal pain. A suggested novel approach for the management of orofacial pain, for clinicians, is presented.\\nManagement of chronic orofacial pain continues to be a major challenge to the clinician.Patients are often placed on a multitude of medications in an attempt to alleviate pain without success.Topical 5% lidocaine plasters, currently used for the management of post-herpetic neuralgia, offer the option of locally targeting trigeminal pain without the multiple side-effects of systemic medication.This case series demonstrates that lidocaine plasters decrease verbal pain scores in extraoral, trigeminal and neuropathic pain, and reduce the use of other neuromodulatory agents in some, but not all, patients.The plasters should be considered as a useful adjuvant in the management of pain in these patients.This case series demonstrates that of the use of 5% lidocaine plasters may play a useful role in the management of chronic trigeminal pain. A suggested novel approach for the management of orofacial pain, for clinicians, is presented.Fourteen patients with chronic orofacial pain conditions referred to the oral surgery department were instructed to wear 5% lidocaine plasters for 12 hours each day over the painful area. The conditions included post-surgical neuropathy (n = 10), multiple sclerosis-related pain (n = 1), persistent idiopathic facial pain (n = 1), Ramsay Hunt syndrome (post-herpetic neuralgia, n = 1) and trigeminal neuralgia (n = 1). Data were collected on patient demographics, pain levels and medication.Pain levels improved in 12 out of 14 patients. Nine patients had a reduction in adjuvant medication, two of whom completely stopped adjuvant treatment.' ,\"The dichotomous role of the gut microbiome in exacerbating and ameliorating neurodegenerative disorders. Age related neurodegenerative disorders affect millions of people around the world. The role of the gut microbiome (GM) in neurodegenerative disorders has been elucidated over the past few years. Dysbiosis of the gut microbiome ultimately results in neurodegeneration. However, the gut microbiome can be modulated to promote neuro-resilience.\\nThis review is focused on demonstrating the role of the gut microbiome in host physiology in Parkinson's disease (PD) and other neurodegenerative disorders. We will discuss how the microbiome will impact neurodegeneration in PD, Alzheimer's Disease (AD), Multiple sclerosis (MS), Amyotrophic Lateral Sclerosis (ALS), and finally discuss how the gut microbiome can be influenced through diet and lifestyle.\\nCurrently, much of the focus has been to study the mechanisms by which the microbiome induces neuroinflammation and neurodegeneration in PD, AD, MS, ALS. In particular, the role of certain dietary flavonoids in regulation of gut microbiome to promote neuro-resilience. Polyphenol prebiotics delivered in combination with probiotics (synbiotics) present an exciting new avenue to harness the microbiome to attenuate immune inflammatory responses which ultimately may influence brain cascades associated with promotion of neurodegeneration across the lifespan.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences0 = [\"The dichotomous role of the gut microbiome in exacerbating and ameliorating neurodegenerative disorders. Age related neurodegenerative disorders affect millions of people around the world. The role of the gut microbiome (GM) in neurodegenerative disorders has been elucidated over the past few years. Dysbiosis of the gut microbiome ultimately results in neurodegeneration. However, the gut microbiome can be modulated to promote neuro-resilience.\\nThis review is focused on demonstrating the role of the gut microbiome in host physiology in Parkinson's disease (PD) and other neurodegenerative disorders. We will discuss how the microbiome will impact neurodegeneration in PD, Alzheimer's Disease (AD), Multiple sclerosis (MS), Amyotrophic Lateral Sclerosis (ALS), and finally discuss how the gut microbiome can be influenced through diet and lifestyle.\\nCurrently, much of the focus has been to study the mechanisms by which the microbiome induces neuroinflammation and neurodegeneration in PD, AD, MS, ALS. In particular, the role of certain dietary flavonoids in regulation of gut microbiome to promote neuro-resilience. Polyphenol prebiotics delivered in combination with probiotics (synbiotics) present an exciting new avenue to harness the microbiome to attenuate immune inflammatory responses which ultimately may influence brain cascades associated with promotion of neurodegeneration across the lifespan.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagss2 = tagger2(sentences0,model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainexaples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-f963ff0ec9a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainexaples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'trainexaples' is not defined"
     ]
    }
   ],
   "source": [
    "test_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt1 = kphext2(sentences,tagss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  '<eos>'],\n",
       " ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'I',\n",
       "  'I',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  '<eos>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>',\n",
       "  '<pad>']]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagss = tagger2(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tagger.model.state_dict(), './kph/lstmcrf.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = svoc.tokenizer('neuro - resilience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neuro - resilience'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(token.text_with_ws for token in A11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt2 = datatrain.iloc[3,:].keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alzheimer’s disease',\n",
       " 'Parkinson’s disease',\n",
       " 'gut dysbiosis',\n",
       " 'gut microbiome',\n",
       " 'neuro-resilience',\n",
       " 'neurodegenerative disorders',\n",
       " 'polyphenols',\n",
       " 'probiotics',\n",
       " 'synbiotics']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gut microbiome\n",
      "neuro-resilience\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for kp in tt2:\n",
    "    if kp.lower() in [x.lower() for x in tt1[0]]:\n",
    "        print(kp)\n",
    "        k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parkinson’s disease'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt2[1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"parkinson's disease\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1[0][6].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Parkinson’s disease'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Parkinson's disease\""
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt1[0][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parkinson’s disease"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svoc(tt2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The dichotomous role of the gut microbiome in exacerbating and ameliorating neurodegenerative disorders. Age related neurodegenerative disorders affect millions of people around the world. The role of the gut microbiome (GM) in neurodegenerative disorders has been elucidated over the past few years. Dysbiosis of the gut microbiome ultimately results in neurodegeneration. However, the gut microbiome can be modulated to promote neuro-resilience.\\nThis review is focused on demonstrating the role of the gut microbiome in host physiology in Parkinson's disease (PD) and other neurodegenerative disorders. We will discuss how the microbiome will impact neurodegeneration in PD, Alzheimer's Disease (AD), Multiple sclerosis (MS), Amyotrophic Lateral Sclerosis (ALS), and finally discuss how the gut microbiome can be influenced through diet and lifestyle.\\nCurrently, much of the focus has been to study the mechanisms by which the microbiome induces neuroinflammation and neurodegeneration in PD, AD, MS, ALS. In particular, the role of certain dietary flavonoids in regulation of gut microbiome to promote neuro-resilience. Polyphenol prebiotics delivered in combination with probiotics (synbiotics) present an exciting new avenue to harness the microbiome to attenuate immune inflammatory responses which ultimately may influence brain cascades associated with promotion of neurodegeneration across the lifespan.\""
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatrain.iloc[3,:].SRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaltest(valid_examples, df_val_k, model):\n",
    "    # This method applies the trained model to a list of sentences.\n",
    "    \n",
    "    # First, create a torchtext Dataset containing the sentences to tag.\n",
    "    iterator = torchtext.data.BucketIterator(\n",
    "        valid_examples,\n",
    "        device=device,\n",
    "        batch_size=64,\n",
    "        repeat=False,\n",
    "        train=False,\n",
    "        sort=False)\n",
    "\n",
    "    # Apply the trained model to all batches.\n",
    "    out = []\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        # Call the model's predict method. This returns a list of NumPy matrix\n",
    "        # containing the integer-encoded tags for each sentence.\n",
    "        predicted = model.predict(batch.text)\n",
    "\n",
    "        # Convert the integer-encoded tags to tag strings.\n",
    "        #for tokens, pred_sen in zip(sentences, predicted):\n",
    "        for tokens, pred_sen in zip(sentences, predicted):\n",
    "            out.append([LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaltest2(df_val, df_val_k, model):\n",
    "    # This method applies the trained model to a list of sentences.\n",
    "    examples = []\n",
    "    for sen in df_val.SRC:\n",
    "        words = tokenizersrc(sen)\n",
    "        labels = ['O']*len(words) # placeholder\n",
    "        examples.append(torchtext.data.Example.fromlist([words, labels], fields))\n",
    "    dataset = torchtext.data.Dataset(examples, fields)\n",
    "\n",
    "    iterator = torchtext.data.Iterator(\n",
    "        dataset,\n",
    "        device=device,\n",
    "        batch_size=1,\n",
    "        repeat=False,\n",
    "        train=False,\n",
    "        sort=False)\n",
    "\n",
    "    # Apply the trained model to all batches.\n",
    "    out = []\n",
    "    model.eval()\n",
    "    for batch in iterator:\n",
    "        # Call the model's predict method. This returns a list of NumPy matrix\n",
    "        # containing the integer-encoded tags for each sentence.\n",
    "        predicted = model.predict(batch.text)\n",
    "\n",
    "        # Convert the integer-encoded tags to tag strings.\n",
    "        #for tokens, pred_sen in zip(sentences, predicted):\n",
    "        for tokens, pred_sen in zip(batch.text.view(1,-1), predicted):\n",
    "            out.append([LABEL.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = evaltest(valid_examples, df_val_k, model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = evaltest2(df_val, df_val_k, model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagperct(df_val,out):\n",
    "    tp = np.empty(len(out))\n",
    "    for i in range(len(df_val.index)):\n",
    "        trg = tokenizertrg(df_val.iloc[i,1])\n",
    "        total = 0\n",
    "        for x in trg:\n",
    "            if x != 'O':\n",
    "                total = total+1\n",
    "        matched = 0\n",
    "        for j in range(total):\n",
    "            if trg[j] != 'O':\n",
    "                if trg[j]== out[i][j]:\n",
    "                    matched = matched +1\n",
    "        p = matched/total\n",
    "        tp[i] = p\n",
    "    return tp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outags2 = kphext2(sentences,tagss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kphperct(df_val_k,out):\n",
    "    tp = np.empty(len(out))\n",
    "    for i in range(len(df_val_k.index)):\n",
    "        ktrg = df_val_k.iloc[i,2]\n",
    "        pred = kphext2([df_val_k.iloc[i,0]],[out[i]])\n",
    "        k = 0\n",
    "        for kp in ktrg:\n",
    "            if str(kp).lower() in [str(x).lower() for x in pred[0]]:\n",
    "                k = k+1\n",
    "        tp[i] = k/len(ktrg)\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lidocaine\n",
      "pain\n"
     ]
    }
   ],
   "source": [
    "ktrg = df_val_k.iloc[0,2]\n",
    "pred = kphext2([df_val_k.iloc[0,0]],[out2[0]])\n",
    "k = 0\n",
    "for kp in ktrg:\n",
    "    if str(kp).lower() in [str(x).lower() for x in pred[0]]:\n",
    "        print(kp)\n",
    "        k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chronic', 'lidocaine', 'neuropathic', 'pain', 'topical', 'trigeminal']"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['trigeminal neuropathic pain',\n",
       "  'lidocaine',\n",
       "  'Chronic trigeminal pain',\n",
       "  'Systemic sodium channel',\n",
       "  'lidocaine',\n",
       "  'pain',\n",
       "  'neuropathic pain',\n",
       "  'oral surgery',\n",
       "  'post-surgical neuropathy',\n",
       "  'facial pain',\n",
       "  'post-herpetic neuralgia',\n",
       "  'Pain',\n",
       "  'lidocaine',\n",
       "  'chronic trigeminal pain',\n",
       "  'post-herpetic neuralgia',\n",
       "  'trigeminal pain',\n",
       "  'neuropathic pain',\n",
       "  'lidocaine',\n",
       "  'trigeminal pain',\n",
       "  'oral surgery',\n",
       "  'post-surgical neuropathy',\n",
       "  'facial pain']]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [df_val_k.iloc[1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9392870656305853"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttp.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp2 = tagperct(df_val,out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15309184079214003"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttp2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttp3 = kphperct(df_val_k,out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3915932524942566"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttp3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024907155236444666"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttp3.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in trg:\n",
    "    if x != 'O':\n",
    "        total = total+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B O O B O O O O O O O O O O B B O O O O O O O O O O O O O O B O O O O O O O O O O O O O O O O O O O O O B O O B B O O O O O O O O O O O O O O O O O O O O O O O O O B O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B O O O O B O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B B O O O O O O O O O O O O O O O O O O O O O O O O B O O B O O O O O O O O O O O O O O O O O B O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O B O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.iloc[1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SRC</th>\n",
       "      <th>TRG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26516508</th>\n",
       "      <td>Case studies illustrating the management of tr...</td>\n",
       "      <td>O O O O O O B B B O B O O B O O B B B O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31374461</th>\n",
       "      <td>Cognitive and brain reserve in multiple sclero...</td>\n",
       "      <td>B O O B O O O O O O O O O O B B O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27717695</th>\n",
       "      <td>B cells of multiple sclerosis patients induce ...</td>\n",
       "      <td>B I O B I O O O O O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31602359</th>\n",
       "      <td>2019 European Thyroid Association Guidelines o...</td>\n",
       "      <td>O B B I B O O B O B O O B I O O B O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24419020</th>\n",
       "      <td>Caregiver burden, quality of life and walking ...</td>\n",
       "      <td>B B O B I I O O O O O B O O B I O O O O B I O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27472871</th>\n",
       "      <td>TRAIL gene expression analysis in multiple scl...</td>\n",
       "      <td>B B I O O B I O O B I O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24347169</th>\n",
       "      <td>Ninjurin1 deficiency attenuates susceptibility...</td>\n",
       "      <td>B O O O O O O O O O O B O O O O O O O O B I O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27631876</th>\n",
       "      <td>High-Resolution Expression Profiling of Periph...</td>\n",
       "      <td>O O O O O O B I O B O O B O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25326018</th>\n",
       "      <td>Effects of coenzyme Q10 on the ratio of TH1/TH...</td>\n",
       "      <td>O O B I O O O O O O O O B I I O O B I O O O B ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30662276</th>\n",
       "      <td>Health-related outcomes, health care resource ...</td>\n",
       "      <td>O O O O O B I I I O O O O O O O O O O O O O O ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2885 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        SRC  \\\n",
       "26516508  Case studies illustrating the management of tr...   \n",
       "31374461  Cognitive and brain reserve in multiple sclero...   \n",
       "27717695  B cells of multiple sclerosis patients induce ...   \n",
       "31602359  2019 European Thyroid Association Guidelines o...   \n",
       "24419020  Caregiver burden, quality of life and walking ...   \n",
       "...                                                     ...   \n",
       "27472871  TRAIL gene expression analysis in multiple scl...   \n",
       "24347169  Ninjurin1 deficiency attenuates susceptibility...   \n",
       "27631876  High-Resolution Expression Profiling of Periph...   \n",
       "25326018  Effects of coenzyme Q10 on the ratio of TH1/TH...   \n",
       "30662276  Health-related outcomes, health care resource ...   \n",
       "\n",
       "                                                        TRG  \n",
       "26516508  O O O O O O B B B O B O O B O O B B B O O O O ...  \n",
       "31374461  B O O B O O O O O O O O O O B B O O O O O O O ...  \n",
       "27717695  B I O B I O O O O O O O O O O O O O O O O O O ...  \n",
       "31602359  O B B I B O O B O B O O B I O O B O O O O O O ...  \n",
       "24419020  B B O B I I O O O O O B O O B I O O O O B I O ...  \n",
       "...                                                     ...  \n",
       "27472871  B B I O O B I O O B I O O O O O O O O O O O O ...  \n",
       "24347169  B O O O O O O O O O O B O O O O O O O O B I O ...  \n",
       "27631876  O O O O O O B I O B O O B O O O O O O O O O O ...  \n",
       "25326018  O O B I O O O O O O O O B I I O O B I O O O B ...  \n",
       "30662276  O O O O O B I I I O O O O O O O O O O O O O O ...  \n",
       "\n",
       "[2885 rows x 2 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kph2",
   "language": "python",
   "name": "kph2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
