{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torchtext\n",
    "import torch\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "svoc = en_core_web_sm.load()\n",
    "import numpy as np\n",
    "from torchtext.data import TabularDataset\n",
    "from torchtext import vocab\n",
    "from torch import nn\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datao = pd.read_pickle(\"~/OneDrive/kph/processed2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     5403\n",
       "4     4737\n",
       "2     3903\n",
       "5     2703\n",
       "1     1883\n",
       "6     1032\n",
       "0      746\n",
       "7      305\n",
       "8      120\n",
       "9       46\n",
       "10      35\n",
       "11      17\n",
       "13       8\n",
       "12       6\n",
       "15       5\n",
       "14       3\n",
       "16       3\n",
       "17       2\n",
       "22       2\n",
       "18       1\n",
       "Name: ext perc, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter out less interesting keywords\n",
    "datao['ext perc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose everything larger than 3 as the training and testing data set\n",
    "datatrain = datao[datao['ext perc']>=3]\n",
    "datatest = datao[datao['ext perc']<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the train, test, validate data set\n",
    "def prepare_csv(df_train, df_test,VAL_RATIO, seed=250):\n",
    "    idx = np.arange(df_train.shape[0])\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(idx)\n",
    "    val_size = int(len(idx) * VAL_RATIO)\n",
    "    df_train.iloc[idx[val_size:], :].to_csv(\n",
    "        \"~/OneDrive/kph/ttd/train.csv\", index=False)\n",
    "    df_train.iloc[idx[:val_size], :].to_csv(\n",
    "        \"~/OneDrive/kph/ttd/val.csv\", index=False)\n",
    "    df_test.to_csv(\"~/OneDrive/kph/ttd/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_csv(datatrain.loc[:,['SRC','TRG']], datatest.loc[:,['SRC','TRG']],0.2, seed=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and validate \n",
    "VAL_RATIO = 0.2\n",
    "dtrain = datatrain.loc[:,['SRC','TRG']]\n",
    "seed=250\n",
    "idx = np.arange(datatrain.shape[0])\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(idx)\n",
    "val_size = int(len(idx) * VAL_RATIO)\n",
    "df_train = dtrain.iloc[idx[val_size:], :]\n",
    "df_val = dtrain.iloc[idx[:val_size], :]\n",
    "df_test = datatest.loc[:,['SRC','TRG']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizertrg = lambda x: x.split()\n",
    "def tokenizersrc(text): # create a tokenizer function\n",
    "    return [tok.text for tok in svoc.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change strategy, store lists directly into the iterator\n",
    "SRC = torchtext.data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, lower=False)\n",
    "TRG = torchtext.data.Field(init_token='<bos>', eos_token='<eos>', sequential=True, unk_token=None)\n",
    "fields = [('SRC', SRC), ('TRG', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "words = []\n",
    "labels = []\n",
    "for pmid in df_train.index:\n",
    "    words = tokenizersrc(df_train.loc[pmid,'SRC'])\n",
    "    labels = tokenizertrg(df_train.loc[pmid,'TRG'])\n",
    "    examples.append(torchtext.data.Example.fromlist([words, labels], fields))\n",
    "trainexamples = torchtext.data.Dataset(examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "words = []\n",
    "labels = []\n",
    "for pmid in df_val.index:\n",
    "    words = tokenizersrc(df_val.loc[pmid,'SRC'])\n",
    "    labels = tokenizertrg(df_val.loc[pmid,'TRG'])\n",
    "    examples.append(torchtext.data.Example.fromlist([words, labels], fields))\n",
    "valexamples = torchtext.data.Dataset(examples, fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embvec = vocab.GloVe(name='840B', dim=300,cache='/home/pding/Documents/glove/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(trainexamples, vectors=embvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(trainexamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_field, label_field, emb_dim, rnn_size, update_pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        voc_size = len(text_field.vocab)\n",
    "        self.n_labels = len(label_field.vocab)       \n",
    "        \n",
    "        # Embedding layer. If we're using pre-trained embeddings, copy them\n",
    "        # into our embedding module.\n",
    "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
    "        if text_field.vocab.vectors is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(text_field.vocab.vectors, \n",
    "                                                       requires_grad=update_pretrained)\n",
    "\n",
    "        # RNN layer. We're using a bidirectional GRU with one layer.\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_size, \n",
    "                          bidirectional=True, num_layers=1)\n",
    "\n",
    "        # Output layer. As in the example last week, the input will be two times\n",
    "        # the RNN size since we are using a bidirectional RNN.\n",
    "        self.top_layer = nn.Linear(2*rnn_size, self.n_labels)\n",
    " \n",
    "        # To deal with the padding positions later, we need to know the\n",
    "        # encoding of the padding dummy word and the corresponding dummy output tag.\n",
    "        self.pad_word_id = text_field.vocab.stoi[text_field.pad_token]\n",
    "        self.pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
    "    \n",
    "        # Loss function that we will use during training.\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "        \n",
    "    def compute_outputs(self, sentences):\n",
    "        # The words in the documents are encoded as integers. The shape of the documents\n",
    "        # tensor is (max_len, n_docs), where n_docs is the number of documents in this batch,\n",
    "        # and max_len is the maximal length of a document in the batch.\n",
    "\n",
    "        # First look up the embeddings for all the words in the documents.\n",
    "        # The shape is now (max_len, n_sentences, emb_dim).        \n",
    "        embedded = self.embedding(sentences)\n",
    "\n",
    "        # Apply the RNN.\n",
    "        # The shape of the RNN output tensor is (max_len, n_sentences, 2*rnn_size).\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Apply the linear output layer.\n",
    "        # The shape of the output tensor is (max_len, n_sentences, n_labels).\n",
    "        out = self.top_layer(rnn_out)\n",
    "        \n",
    "        # Find the positions where the token is a dummy padding token.\n",
    "        pad_mask = (sentences == self.pad_word_id).float()\n",
    "\n",
    "        # For these positions, we add some large number in the column corresponding\n",
    "        # to the dummy padding label.\n",
    "        out[:, :, self.pad_label_id] += pad_mask*10000\n",
    "\n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels):\n",
    "        # As discussed above, this method first computes the predictions, and then\n",
    "        # the loss function.\n",
    "        \n",
    "        # Compute the outputs. The shape is (max_len, n_sentences, n_labels).\n",
    "        scores = self.compute_outputs(sentences)\n",
    "        \n",
    "        # Flatten the outputs and the gold-standard labels, to compute the loss.\n",
    "        # The input to this loss needs to be one 2-dimensional and one 1-dimensional tensor.\n",
    "        scores = scores.view(-1, self.n_labels)\n",
    "        labels = labels.view(-1)\n",
    "        return self.loss(scores, labels)\n",
    "\n",
    "    def predict(self, sentences):\n",
    "        # Compute the outputs from the linear units.\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # Select the top-scoring labels. The shape is now (max_len, n_sentences).\n",
    "        predicted = scores.argmax(dim=2)\n",
    "\n",
    "        # We transpose the prediction to (n_sentences, max_len), and convert it\n",
    "        # to a NumPy matrix.\n",
    "        return predicted.t().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchcrf import CRF\n",
    "\n",
    "class RNNCRFTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, text_field, label_field, emb_dim, rnn_size, update_pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        voc_size = len(text_field.vocab)\n",
    "        self.n_labels = len(label_field.vocab)       \n",
    "        \n",
    "        self.embedding = nn.Embedding(voc_size, emb_dim)\n",
    "        if text_field.vocab.vectors is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(text_field.vocab.vectors, \n",
    "                                                       requires_grad=update_pretrained)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=rnn_size, \n",
    "                          bidirectional=True, num_layers=1)\n",
    "\n",
    "        self.top_layer = nn.Linear(2*rnn_size, self.n_labels)\n",
    " \n",
    "        self.pad_word_id = text_field.vocab.stoi[text_field.pad_token]\n",
    "        self.pad_label_id = label_field.vocab.stoi[label_field.pad_token]\n",
    "    \n",
    "        self.crf = CRF(self.n_labels)\n",
    "        \n",
    "    def compute_outputs(self, sentences):\n",
    "        embedded = self.embedding(sentences)\n",
    "        rnn_out, _ = self.rnn(embedded)\n",
    "        out = self.top_layer(rnn_out)\n",
    "        \n",
    "        pad_mask = (sentences == self.pad_word_id).float()\n",
    "        out[:, :, self.pad_label_id] += pad_mask*10000\n",
    "        \n",
    "        return out\n",
    "                \n",
    "    def forward(self, sentences, labels):\n",
    "        # Compute the outputs of the lower layers, which will be used as emission\n",
    "        # scores for the CRF.\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # We return the loss value. The CRF returns the log likelihood, but we return \n",
    "        # the *negative* log likelihood as the loss value.            \n",
    "        # PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\n",
    "        # log likelihood.\n",
    "        return -self.crf(scores, labels)\n",
    "            \n",
    "    def predict(self, sentences):\n",
    "        # Compute the emission scores, as above.\n",
    "        scores = self.compute_outputs(sentences)\n",
    "\n",
    "        # Apply the Viterbi algorithm to get the predictions. This implementation returns\n",
    "        # the result as a list of lists (not a tensor), corresponding to a matrix\n",
    "        # of shape (n_sentences, max_len).\n",
    "        return self.crf.decode(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a list of BIO labels, coded as integers, into spans identified by a beginning, an end, and a label.\n",
    "# To allow easy comparison later, we store them in a dictionary indexed by the start position.\n",
    "def to_spans(l_ids, voc):\n",
    "    spans = {}\n",
    "    current_lbl = None\n",
    "    current_start = None\n",
    "    for i, l_id in enumerate(l_ids):\n",
    "        l = voc[l_id]\n",
    "\n",
    "        if l[0] == 'B': \n",
    "            # Beginning of a named entity: B-something.\n",
    "            if current_lbl:\n",
    "                # If we're working on an entity, close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "            # Create a new entity that starts here.\n",
    "            current_lbl = l[2:]\n",
    "            current_start = i\n",
    "        elif l[0] == 'I':\n",
    "            # Continuation of an entity: I-something.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, but its label does not\n",
    "                # correspond to the predicted I-tag, then we close\n",
    "                # the open entity and create a new one.\n",
    "                if current_lbl != l[2:]:\n",
    "                    spans[current_start] = (current_lbl, i)\n",
    "                    current_lbl = l[2:]\n",
    "                    current_start = i\n",
    "            else:\n",
    "                # If we don't have an open entity but predict an I tag,\n",
    "                # we create a new entity starting here even though we're\n",
    "                # not following the format strictly.\n",
    "                current_lbl = l[2:]\n",
    "                current_start = i\n",
    "        else:\n",
    "            # Outside: O.\n",
    "            if current_lbl:\n",
    "                # If we have an open entity, we close it.\n",
    "                spans[current_start] = (current_lbl, i)\n",
    "                current_lbl = None\n",
    "                current_start = None\n",
    "    return spans\n",
    "\n",
    "# Compares two sets of spans and records the results for future aggregation.\n",
    "def compare(gold, pred, stats):\n",
    "    for start, (lbl, end) in gold.items():\n",
    "        stats['total']['gold'] += 1\n",
    "        stats[lbl]['gold'] += 1\n",
    "    for start, (lbl, end) in pred.items():\n",
    "        stats['total']['pred'] += 1\n",
    "        stats[lbl]['pred'] += 1\n",
    "    for start, (glbl, gend) in gold.items():\n",
    "        if start in pred:\n",
    "            plbl, pend = pred[start]\n",
    "            if glbl == plbl and gend == pend:\n",
    "                stats['total']['corr'] += 1\n",
    "                stats[glbl]['corr'] += 1\n",
    "\n",
    "# This function combines the auxiliary functions we defined above.\n",
    "def evaluate_iob(predicted, gold, label_field, stats):\n",
    "    # The gold-standard labels are assumed to be an integer tensor of shape\n",
    "    # (max_len, n_sentences), as returned by torchtext.\n",
    "    gold_cpu = gold.t().cpu().numpy()\n",
    "    gold_cpu = list(gold_cpu.reshape(-1))\n",
    "\n",
    "    # The predicted labels assume the format produced by pytorch-crf, so we\n",
    "    # assume that they have been converted into a list already.\n",
    "    # We just flatten the list.\n",
    "    pred_cpu = [l for sen in predicted for l in sen]\n",
    "    \n",
    "    # Compute spans for the gold standard and prediction.\n",
    "    gold_spans = to_spans(gold_cpu, label_field.vocab.itos)\n",
    "    pred_spans = to_spans(pred_cpu, label_field.vocab.itos)\n",
    "\n",
    "    # Finally, update the counts for correct, predicted and gold-standard spans.\n",
    "    compare(gold_spans, pred_spans, stats)\n",
    "\n",
    "# Computes precision, recall and F-score, given a dictionary that contains\n",
    "# the counts of correct, predicted and gold-standard items.\n",
    "def prf(stats):\n",
    "    if stats['pred'] == 0:\n",
    "        return 0, 0, 0\n",
    "    p = stats['corr']/stats['pred']\n",
    "    r = stats['corr']/stats['gold']\n",
    "    if p > 0 and r > 0:\n",
    "        f = 2*p*r/(p+r)\n",
    "    else:\n",
    "        f = 0\n",
    "    return p, r, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = RNNCRFTagger(SRC, TRG, emb_dim=300, rnn_size=128, update_pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of words and sentences.\n",
    "n_tokens_train = 0\n",
    "n_sentences_train = 0\n",
    "for ex in trainexamples:\n",
    "    n_tokens_train += len(ex.SRC) + 2\n",
    "    n_sentences_train += 1\n",
    "n_tokens_valid = 0       \n",
    "for ex in valexamples:\n",
    "    n_tokens_valid += len(ex.SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 300\n",
    "n_batches = np.ceil(n_sentences_train / batch_size)\n",
    "\n",
    "mean_n_tokens = n_tokens_train / n_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = torchtext.data.BucketIterator(\n",
    "            trainexamples,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "            sort_key=lambda x: len(x.SRC),\n",
    "            repeat=False,\n",
    "            train=True,\n",
    "            sort=True)\n",
    "\n",
    "valid_iterator = torchtext.data.BucketIterator(\n",
    "    valexamples,\n",
    "    device=device,\n",
    "    batch_size=64,\n",
    "    sort_key=lambda x: len(x.SRC),\n",
    "    repeat=False,\n",
    "    train=False,\n",
    "    sort=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = list(train_iterator)\n",
    "valid_batches = list(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNCRFTagger(\n",
       "  (embedding): Embedding(72701, 300)\n",
       "  (rnn): GRU(300, 128, bidirectional=True)\n",
       "  (top_layer): Linear(in_features=256, out_features=6, bias=True)\n",
       "  (crf): CRF(num_tags=6)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "the first two dimensions of emissions and tags must match, got (88, 300) and (90, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-11e09d263fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Compute the output and loss.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSRC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmean_n_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ac7cecedc58f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences, labels)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# PyTorch's optimizers *minimize* the loss, while we want to *maximize* the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# log likelihood.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kph/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kph/lib/python3.7/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mreduction\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mnone\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memissions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'token_mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'invalid reduction: {reduction}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kph/lib/python3.7/site-packages/torchcrf/__init__.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0memissions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 156\u001b[0;31m                     \u001b[0;34m'the first two dimensions of emissions and tags must match, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                     f'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}')\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: the first two dimensions of emissions and tags must match, got (88, 300) and (90, 300)"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model0.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "n_labels = len(TRG.vocab)\n",
    "\n",
    "history = defaultdict(list)    \n",
    "\n",
    "n_epochs = 3\n",
    "\n",
    "for i in range(1, n_epochs + 1):\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    model0.train()\n",
    "    for batch in train_batches:\n",
    "\n",
    "        # Compute the output and loss.\n",
    "        loss = model0(batch.SRC, batch.TRG) / mean_n_tokens\n",
    "\n",
    "        optimizer.zero_grad()            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "    train_loss = loss_sum / n_batches\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n",
    "    # Evaluate on the validation set.\n",
    "    if i % 1 == 0:\n",
    "        stats = defaultdict(Counter)\n",
    "\n",
    "        model0.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_batches:\n",
    "                # Predict the model's output on a batch.\n",
    "                predicted = model0.predict(batch.SRC)                   \n",
    "                # Update the evaluation statistics.\n",
    "                evaluate_iob(predicted, batch.TRG, TRG, stats)\n",
    "\n",
    "        # Compute the overall F-score for the validation set.\n",
    "        _, _, val_f1 = prf(stats['total'])\n",
    "\n",
    "        history['val_f1'].append(val_f1)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f'Epoch {i}: train loss = {train_loss:.4f}, val f1: {val_f1:.4f}, time = {t1-t0:.4f}')\n",
    "\n",
    "# After the final evaluation, we print more detailed evaluation statistics, including\n",
    "# precision, recall, and F-scores for the different types of named entities.\n",
    "print()\n",
    "print('Final evaluation on the validation set:')\n",
    "p, r, f1 = prf(stats['total'])\n",
    "print(f'Overall: P = {p:.4f}, R = {r:.4f}, F1 = {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(sentences):\n",
    "    # This method applies the trained model to a list of sentences.\n",
    "\n",
    "    # First, create a torchtext Dataset containing the sentences to tag.\n",
    "    examples = []\n",
    "    for sen in sentences:\n",
    "        labels = ['?']*len(sen) # placeholder\n",
    "        examples.append(torchtext.data.Example.fromlist([sen, labels], fields))\n",
    "    dataset = torchtext.data.Dataset(examples, fields)\n",
    "\n",
    "    iterator = torchtext.data.Iterator(\n",
    "        dataset,\n",
    "        device=device,\n",
    "        batch_size=64,\n",
    "        repeat=False,\n",
    "        train=False,\n",
    "        sort=False)\n",
    "\n",
    "    # Apply the trained model to all batches.\n",
    "    out = []\n",
    "    model0.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            # Call the model's predict method. This returns a list of NumPy matrix\n",
    "            # containing the integer-encoded tags for each sentence.\n",
    "            predicted = model0.predict(batch.SRC)\n",
    "\n",
    "            # Convert the integer-encoded tags to tag strings.\n",
    "            for tokens, pred_sen in zip(sentences, predicted):\n",
    "                out.append([self.TRG.vocab.itos[pred_id] for _, pred_id in zip(tokens, pred_sen[1:])])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tags(sentence):\n",
    "    tokens = sentence.split()\n",
    "    tags = tagging([tokens])[0]\n",
    "    for token, tag1 in zip(tokens, tags):\n",
    "        print(f'{token:12s}{tag1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tagging() missing 1 required positional argument: 'sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-2a947173bec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'John Johnson was born in Moscow , lives in Gothenburg , and works for Chalmers Technical University and the University of Gothenburg .'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-c5d0a06963bf>\u001b[0m in \u001b[0;36mprint_tags\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{token:12s}{tag1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tagging() missing 1 required positional argument: 'sentences'"
     ]
    }
   ],
   "source": [
    "print_tags('John Johnson was born in Moscow , lives in Gothenburg , and works for Chalmers Technical University and the University of Gothenburg .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(sequential=True, tokenize=tokenizersrc)\n",
    "TRG = Field(sequential=True, tokenize=tokenizertrg, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('SRC', SRC), ('TRG', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = TabularDataset.splits(\n",
    "    path='~/OneDrive/kph/ttd/', train='train.csv',\n",
    "    validation='val.csv', test='test.csv', format='csv',\n",
    "    fields=data_fields,skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first version with fixed embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train, vectors=embvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efdcc0c07b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT.vocab.set_vectors(vectors.stoi, vectors.vectors, vectors.dim)\n",
    "#embedding = nn.Embedding(n_embed, embed_dim).from_pretrained(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size = BATCH_SIZE)\n",
    "    #sort_key=lambda x: len(x.SRC))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keyphrases extraction",
   "language": "python",
   "name": "kphextr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
