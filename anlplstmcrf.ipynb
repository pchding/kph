{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "from allennlp.models import crf_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.params import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules import  ConditionalRandomField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for PoS tagging data, one sentence per line, like\n",
    "        The###DET dog###NN ate###V the###DET apple###NN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "\n",
    "        if tags:\n",
    "            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n",
    "            fields[\"labels\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                pairs = line.strip().split()\n",
    "                sentence, tags = zip(*(pair.split(\"###\") for pair in pairs))\n",
    "                yield self.text_to_instance([Token(word) for word in sentence], tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmCRFTagger(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.crf = ConditionalRandomField(num_tags=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmTagger(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11543it [00:08, 1309.54it/s]\n",
      "2885it [00:02, 1339.03it/s]\n",
      "100%|██████████| 14428/14428 [00:03<00:00, 4553.81it/s]\n"
     ]
    }
   ],
   "source": [
    "reader = PosDatasetReader()\n",
    "train_dataset = reader.read(\n",
    "    '/home/pding/OneDrive/kph/kph/trainan.txt')\n",
    "validation_dataset = reader.read(\n",
    "    '/home/pding/OneDrive/kph/kph/testan.txt')\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [00:13, 161347.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "#                            embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "token_embedding = Embedding.from_params(\n",
    "                            vocab=vocab,\n",
    "                            params=Params({'pretrained_file':'/home/pding/Documents/glove/glove.840B.300d.txt',\n",
    "                                           'embedding_dim' : EMBEDDING_DIM})\n",
    "                            )\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save_to_files(\"/tmp/vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LstmTagger(word_embeddings, lstm, vocab)\n",
    "model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "iterator = BucketIterator(batch_size=300, biggest_batch_first=True, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  cuda_device=0,\n",
    "                  patience=10,\n",
    "                  num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LstmCRFTagger(word_embeddings, lstm, vocab)\n",
    "model2.cuda()\n",
    "optimizer = optim.SGD(model2.parameters(), lr=0.01)\n",
    "iterator = BucketIterator(batch_size=300, biggest_batch_first=True, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer2 = Trainer(model=model2,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  cuda_device=0,\n",
    "                  patience=10,\n",
    "                  num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model2.th\", 'rb') as f:\n",
    "    model2.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BucketIterator' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-719dd7955152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'BucketIterator' object is not iterable"
     ]
    }
   ],
   "source": [
    "next(iter(iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9384, loss: 0.3140 ||: 100%|██████████| 39/39 [00:06<00:00,  5.88it/s]\n",
      "accuracy: 0.9375, loss: 0.3160 ||: 100%|██████████| 10/10 [00:00<00:00, 10.33it/s]\n",
      "accuracy: 0.9384, loss: 0.3125 ||: 100%|██████████| 39/39 [00:06<00:00,  5.95it/s]\n",
      "accuracy: 0.9375, loss: 0.3146 ||: 100%|██████████| 10/10 [00:00<00:00, 10.64it/s]\n",
      "accuracy: 0.9384, loss: 0.3111 ||: 100%|██████████| 39/39 [00:06<00:00,  5.96it/s]\n",
      "accuracy: 0.9375, loss: 0.3134 ||: 100%|██████████| 10/10 [00:00<00:00, 10.68it/s]\n",
      "accuracy: 0.9384, loss: 0.3099 ||: 100%|██████████| 39/39 [00:06<00:00,  5.98it/s]\n",
      "accuracy: 0.9375, loss: 0.3121 ||: 100%|██████████| 10/10 [00:00<00:00, 10.60it/s]\n",
      "accuracy: 0.9434, loss: 0.2783 ||:  33%|███▎      | 13/39 [00:03<00:04,  5.39it/s]"
     ]
    }
   ],
   "source": [
    "trainer2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model2.th\", 'wb') as f:\n",
    "    torch.save(model2.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spacy models 'en_core_web_sm' not found.  Downloading and installing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/pding/anaconda3/envs/anlp/lib/python3.7/site-packages/en_core_web_sm -->\n",
      "/home/pding/anaconda3/envs/anlp/lib/python3.7/site-packages/spacy/data/en_core_web_sm\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "['O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "tag_logits = predictor.predict(\"The dog ate the apple\")['tag_logits']\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
